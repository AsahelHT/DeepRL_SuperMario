
# ğŸ•¹ï¸ğŸ‘¾ Reinforcement Learning aplicado a *Super Mario Bros* ğŸ‘¾ğŸ•¹ï¸

---

## ğŸ¯ PresentaciÃ³n de la prÃ¡ctica

Se entrega un **notebook modificado** para facilitar la comprensiÃ³n y ejecuciÃ³n de la prÃ¡ctica desarrollada.  
Este cuaderno ha sido adaptado eliminando las celdas de los tutoriales de ejemplo y aÃ±adiendo nuevas secciones para mejorar el flujo de trabajo y el entendimiento del cÃ³digo.

---

## ğŸ Resultado obtenido

<figure style="text-align: center; margin-top: 20px;">
  <img src="./media/gifs/PPO_CNN_3005_3.gif" alt="Mejor entrenamiento logrado" width="350">
  <figcaption style="font-style: italic; color: #555; margin-top: 10px;">
    ğŸ† Mejor entrenamiento logrado: <strong>PPO_CNN_3005_3</strong>
  </figcaption>
</figure>

---

## ğŸ“¦ Estructura del notebook

El notebook se organiza en **cuatro secciones principales**:

1. âš™ï¸ **ConfiguraciÃ³n del entorno**  
   PreparaciÃ³n del entorno con Gym y SuperMarioBros, junto con wrappers personalizados.
2. ğŸ› ï¸ **ModificaciÃ³n y ampliaciÃ³n**  
   ImplementaciÃ³n de funciones de recompensa, normalizaciÃ³n, y stack de frames.
3. ğŸ§  **Entrenamiento del agente**  
   Uso de PPO con `CnnPolicy`, ajuste de hiperparÃ¡metros y visualizaciÃ³n con TensorBoard.
4. ğŸ“ˆ **EvaluaciÃ³n de desempeÃ±o**  
   EvaluaciÃ³n cuantitativa y cualitativa con grabaciÃ³n de episodios como GIFs.

---

## ğŸ’» Requisitos

- Python 3.8+
- Stable-Baselines3
- Gym SuperMarioBros
- OpenCV
- Matplotlib
- Tensorboard

---

## ğŸš€ CÃ³mo ejecutar

1. Clona el repositorio.
2. Crea y activa un entorno virtual:  
   `python -m venv .venv && source .venv/bin/activate` (Linux/Mac)  
   `.\.venv\Scripts\activate` (Windows)
3. Instala dependencias:  
   `pip install -r requirements.txt`
4. Ejecuta el notebook:  
   `jupyter notebook mario_rl.ipynb`

---

## ğŸ“‚ Estructura del proyecto

```
ğŸ“ DeepRL_SuperMario/
â”œâ”€â”€ ğŸ“ .venv/                  	   â†’ Entorno virtual de Python
â”œâ”€â”€ ğŸ“ docs/                   	   â†’ DocumentaciÃ³n del proyecto
â”œâ”€â”€ ğŸ“ final_models/           	   â†’ Modelos PPO entrenados 
â”‚   â””â”€â”€ ğŸ“¦ PPO_CNN_2705_1.zip ...
â”œâ”€â”€ ğŸ“ mario_logs/             	   â†’ Logs de entrenamiento
â”œâ”€â”€ ğŸ“ mario_models/           	   â†’ Modelos intermedios 
â”œâ”€â”€ ğŸ“ mario_monitor_dir/      	   â†’ InformaciÃ³n de episodios
â”œâ”€â”€ ğŸ“ media/                  	   â†’ Gifs e imÃ¡genes 
â”‚   â”œâ”€â”€ ğŸ“ gifs/    			         â†’ Render de los agentes jugando
â”‚   â”‚   â””â”€â”€	ğŸï¸ PPO_CNN_2705_2.gif ...	
â”‚   â””â”€â”€ ğŸ“ images/    			      â†’ Output finales de los entrenamientos
â”‚       â””â”€â”€	ğŸ–¼ï¸ PPO_CNN_2705_2.png ...
â”œâ”€â”€ ğŸ“ src/                    	   â†’ CÃ³digo de backup
â”œâ”€â”€ ğŸ“¦ mario_final_model.zip   	   â†’ Modelo PPO final entrenado
â”œâ”€â”€ ğŸ““ mario_rl.ipynb           	   â†’ Notebook de la prÃ¡ctica
â””â”€â”€ ğŸ“„ requirements.txt        	   â†’ Dependencias del entorno

```

---

## âœ¨ AutorÃ­a

Desarrollado como prÃ¡ctica acadÃ©mica. Recomendado ver el cÃ³digo completo y ejecutar las pruebas para comprender a fondo el aprendizaje por refuerzo aplicado a videojuegos clÃ¡sicos.

---
